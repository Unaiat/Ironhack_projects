{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "import spacy\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "import sqlalchemy as alch\n",
    "from getpass import getpass\n",
    "from config.configuration import engine\n",
    "import src.cleaning as cl\n",
    "import src.get_data as gd\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduce tu pass de sql: ········\n"
     ]
    }
   ],
   "source": [
    "password = getpass(\"Introduce tu pass de sql: \")\n",
    "dbName=\"elections\"\n",
    "connectionData=f\"mysql+pymysql://root:{password}@localhost/{dbName}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conected\n"
     ]
    }
   ],
   "source": [
    "engine = alch.create_engine(connectionData)\n",
    "print(\"Conected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROGRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_programs = gd.get_phrases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba = gd.verba_phrases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/unai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_programs.dropna(axis = 0, how = 'any', inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_programs['tokenized'] = all_programs['phrases'].apply(cl.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/unai/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_programs['tokenized'] = all_programs['tokenized'].apply(cl.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>party_id</th>\n",
       "      <th>phrases</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>El próximo 4 de mayo las madrileñas y madrileñ...</td>\n",
       "      <td>El próximo 4 mayo madrileñas madrileños haber ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Hemos sufrido el abandono de una presidenta qu...</td>\n",
       "      <td>Hemos sufrido abandono presidenta utilizado se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Abordamos esta convocatoria electoral desde el...</td>\n",
       "      <td>Abordamos convocatoria electoral conocimiento ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Los compromisos que adquirimos con el programa...</td>\n",
       "      <td>Los compromisos adquirimos programa electoral ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Hemos revisado y actualizado las medidas para ...</td>\n",
       "      <td>Hemos revisado actualizado medidas soluciones ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3717</th>\n",
       "      <td>3718</td>\n",
       "      <td>4</td>\n",
       "      <td>Colaborar con los ayuntamientos en los program...</td>\n",
       "      <td>Colaborar ayuntamientos programas agentes tuto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3718</th>\n",
       "      <td>3719</td>\n",
       "      <td>4</td>\n",
       "      <td>Elaborar un Plan Director para el Cuerpo de Pr...</td>\n",
       "      <td>Elaborar Plan Director Cuerpo Prevención Extin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3719</th>\n",
       "      <td>3720</td>\n",
       "      <td>4</td>\n",
       "      <td>Mejorar la dotación de personal, aumentando la...</td>\n",
       "      <td>Mejorar dotación personal aumentando plantilla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3720</th>\n",
       "      <td>3721</td>\n",
       "      <td>4</td>\n",
       "      <td>Mejorar la eficacia de la extinción de incend...</td>\n",
       "      <td>Mejorar eficacia extincio n incendios creando ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3721</th>\n",
       "      <td>3722</td>\n",
       "      <td>4</td>\n",
       "      <td>Gobernar en serio.</td>\n",
       "      <td>Gobernar serio</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3722 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  party_id                                            phrases  \\\n",
       "0        1         2  El próximo 4 de mayo las madrileñas y madrileñ...   \n",
       "1        2         2  Hemos sufrido el abandono de una presidenta qu...   \n",
       "2        3         2  Abordamos esta convocatoria electoral desde el...   \n",
       "3        4         2  Los compromisos que adquirimos con el programa...   \n",
       "4        5         2  Hemos revisado y actualizado las medidas para ...   \n",
       "...    ...       ...                                                ...   \n",
       "3717  3718         4  Colaborar con los ayuntamientos en los program...   \n",
       "3718  3719         4  Elaborar un Plan Director para el Cuerpo de Pr...   \n",
       "3719  3720         4  Mejorar la dotación de personal, aumentando la...   \n",
       "3720  3721         4  Mejorar la eficacia de la extinción de incend...   \n",
       "3721  3722         4                                Gobernar en serio.    \n",
       "\n",
       "                                              tokenized  \n",
       "0     El próximo 4 mayo madrileñas madrileños haber ...  \n",
       "1     Hemos sufrido abandono presidenta utilizado se...  \n",
       "2     Abordamos convocatoria electoral conocimiento ...  \n",
       "3     Los compromisos adquirimos programa electoral ...  \n",
       "4     Hemos revisado actualizado medidas soluciones ...  \n",
       "...                                                 ...  \n",
       "3717  Colaborar ayuntamientos programas agentes tuto...  \n",
       "3718  Elaborar Plan Director Cuerpo Prevención Extin...  \n",
       "3719  Mejorar dotación personal aumentando plantilla...  \n",
       "3720  Mejorar eficacia extincio n incendios creando ...  \n",
       "3721                                     Gobernar serio  \n",
       "\n",
       "[3722 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#spanish_stemmer = SnowballStemmer('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spanish_stemmer.stem(\"libertad, libre, libres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spanish_stemmer.stem(\"libertad, libre, libres, libertad, libre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between Stemming and Lematization, the latter works better. However, unfortunately there is no option to do it in Spanish. \n",
    "\n",
    "Although there is the option of translating the text, in this case I consider it important to keep it in the original language. \n",
    "\n",
    "I tried the SnoballStemmer, which is available in Spanish, but as it can be seen above the results are very good. It was supposed to take the root of the word and categorize all as the same. Instead, it puts them all as the same and deletes a part of the last one. \n",
    "\n",
    "Therefore, I will perform the search of the most common words in the column where I applied the tokenization and the stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count words using value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the word count of all the programs together\n",
    "df_word_count = cl.count_values(all_programs, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list with the words to delete (The ones that would appear in the first 30)\n",
    "to_delete = [\"La\", \"Se\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to delete them\n",
    "df_word_count = cl.delete_extra_words(df_word_count, to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_word_count = cl.for_streamlit(df_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Spacy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_common_verbs = cl.spacy_verb(all_programs, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_common_nouns = cl.spacy_noun(all_programs, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>party_id</th>\n",
       "      <th>phrases</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>Queremos una Comunidad de Madrid con una sanid...</td>\n",
       "      <td>Queremos Comunidad Madrid sanidad cuide capaci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>Una Comunidad de Madrid que comprenda que somo...</td>\n",
       "      <td>Una Comunidad Madrid comprenda personas interd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>Garantizaremos que todas las personas de la Co...</td>\n",
       "      <td>Garantizaremos todas personas Comunidad Madrid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>Se reembolsarán los copagos farmacéuticos y no...</td>\n",
       "      <td>Se reembolsarán copagos farmacéuticos farmacéu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>Se implementarán mecanismos para hacer efectiv...</td>\n",
       "      <td>Se implementarán mecanismos hacer efectivas me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3686</th>\n",
       "      <td>3687</td>\n",
       "      <td>4</td>\n",
       "      <td>Otros de los bienes públicos que buscamos gara...</td>\n",
       "      <td>Otros bienes públicos buscamos garantizar segu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3690</th>\n",
       "      <td>3691</td>\n",
       "      <td>4</td>\n",
       "      <td>Nos comprometeremos a la reducción de la tempo...</td>\n",
       "      <td>Nos comprometeremos reducción temporalidad emp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3708</th>\n",
       "      <td>3709</td>\n",
       "      <td>4</td>\n",
       "      <td>Madrid es una de las comunidades con mayor lit...</td>\n",
       "      <td>Madrid comunidades mayor litigiosidad colapsa ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3711</th>\n",
       "      <td>3712</td>\n",
       "      <td>4</td>\n",
       "      <td>Garantizar el acceso a todas las sedes judicia...</td>\n",
       "      <td>Garantizar acceso todas sedes judiciales Comun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3719</th>\n",
       "      <td>3720</td>\n",
       "      <td>4</td>\n",
       "      <td>Mejorar la dotación de personal, aumentando la...</td>\n",
       "      <td>Mejorar dotación personal aumentando plantilla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>494 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  party_id                                            phrases  \\\n",
       "21      22         2  Queremos una Comunidad de Madrid con una sanid...   \n",
       "22      23         2  Una Comunidad de Madrid que comprenda que somo...   \n",
       "29      30         2  Garantizaremos que todas las personas de la Co...   \n",
       "32      33         2  Se reembolsarán los copagos farmacéuticos y no...   \n",
       "34      35         2  Se implementarán mecanismos para hacer efectiv...   \n",
       "...    ...       ...                                                ...   \n",
       "3686  3687         4  Otros de los bienes públicos que buscamos gara...   \n",
       "3690  3691         4  Nos comprometeremos a la reducción de la tempo...   \n",
       "3708  3709         4  Madrid es una de las comunidades con mayor lit...   \n",
       "3711  3712         4  Garantizar el acceso a todas las sedes judicia...   \n",
       "3719  3720         4  Mejorar la dotación de personal, aumentando la...   \n",
       "\n",
       "                                              tokenized  \n",
       "21    Queremos Comunidad Madrid sanidad cuide capaci...  \n",
       "22    Una Comunidad Madrid comprenda personas interd...  \n",
       "29    Garantizaremos todas personas Comunidad Madrid...  \n",
       "32    Se reembolsarán copagos farmacéuticos farmacéu...  \n",
       "34    Se implementarán mecanismos hacer efectivas me...  \n",
       "...                                                 ...  \n",
       "3686  Otros bienes públicos buscamos garantizar segu...  \n",
       "3690  Nos comprometeremos reducción temporalidad emp...  \n",
       "3708  Madrid comunidades mayor litigiosidad colapsa ...  \n",
       "3711  Garantizar acceso todas sedes judiciales Comun...  \n",
       "3719  Mejorar dotación personal aumentando plantilla...  \n",
       "\n",
       "[494 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_programs[all_programs['phrases'].str.contains(\"persona\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the search made using value_counts and Spacy are different. I would say this happens due to errors in the entity recognition of spacy, which doesn't always correctly put the tag (noun, verb...) to the word. \n",
    "\n",
    "I will use both methods (the one with value_counts() and the spacy library to learn how to use it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze each party"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_phrases = gd.phrases(\"PP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column to apply tokenization\n",
    "pp_phrases['tokenized'] = pp_phrases['phrases'].apply(cl.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the stop words in the column \n",
    "pp_phrases['tokenized'] = pp_phrases['tokenized'].apply(cl.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the first function that creates a dataframe with words capitalized and the frequency\n",
    "pp_phrases_count = cl.count_values(pp_phrases, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list with the words to delete\n",
    "to_delete = [\"Años\", \"Ello\", \"En\", \"La\", \"Puedan\", \"Través\", \"Especialmente\", \n",
    "             \"Nuevas\", \"Vamos\", \"Además\", \"Especialmente\", \"Una\", \"Aquellos\", \"Cada\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to delete words\n",
    "pp_phrases_count = cl.delete_extra_words(pp_phrases_count, to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prepare the dataframe to make a graph in streamlit\n",
    "pp_phrases_count = cl.for_streamlit(pp_phrases_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_phrases_count.to_csv(\"./data/programs/csv-s/pp_phrases_count.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_verbs = cl.spacy_verb(pp_phrases, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pp_verbs.to_csv(\"./data/programs/csv-s/pp_verbs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_nouns = cl.spacy_noun(pp_phrases, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pp_nouns.to_csv(\"./data/programs/csv-s/pp_nouns.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PSOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "psoe_phrases = gd.phrases(\"PSOE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "psoe_phrases['tokenized'] = psoe_phrases['phrases'].apply(cl.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "psoe_phrases['tokenized'] = psoe_phrases['tokenized'].apply(cl.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "psoe_phrases_count = cl.count_values(psoe_phrases, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = [\"N\", \"La\", \"Así\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "psoe_phrases_count = cl.delete_extra_words(psoe_phrases_count, to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "psoe_phrases_count = cl.for_streamlit(psoe_phrases_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#psoe_phrases_count.to_csv(\"./data/programs/csv-s/psoe_phrases_count.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "psoe_verbs = cl.spacy_verb(psoe_phrases, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#psoe_verbs.to_csv(\"./data/programs/csv-s/psoe_verbs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "psoe_nouns = cl.spacy_noun(psoe_phrases, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#psoe_nouns.to_csv(\"./data/programs/csv-s/psoe_nouns.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Más Madrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "mas_madrid_phrases = gd.phrases(\"Más Madrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mas_madrid_phrases['tokenized'] = mas_madrid_phrases['phrases'].apply(cl.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mas_madrid_phrases['tokenized'] = mas_madrid_phrases['tokenized'].apply(cl.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mas_madrid_phrases_count = cl.count_values(psoe_phrases, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = [\"Ción\", \"N\", \"La\", \"Dad\", \"Así\", \"Dos\", \"Co\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mas_madrid_phrases_count = cl.delete_extra_words(mas_madrid_phrases_count, to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mas_madrid_phrases_count = cl.for_streamlit(mas_madrid_phrases_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mas_madrid_phrases_count.to_csv(\"./data/programs/csv-s/mas_madrid_phrases_count.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "mas_madrid_verbs = cl.spacy_verb(mas_madrid_phrases, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mas_madrid_verbs.to_csv(\"./data/programs/csv-s/mas_madrid_verbs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "mas_madrid_nouns = cl.spacy_noun(mas_madrid_phrases, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mas_madrid_nouns.to_csv(\"./data/programs/csv-s/mas_madrid_nouns.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unidas Podemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "unidas_podemos_phrases = gd.phrases(\"Unidas Podemos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "unidas_podemos_phrases['tokenized'] = unidas_podemos_phrases['phrases'].apply(cl.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "unidas_podemos_phrases['tokenized'] = unidas_podemos_phrases['tokenized'].apply(cl.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "unidas_podemos_phrases_count = cl.count_values(unidas_podemos_phrases, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = [\"Cada\", \"Ciento\", \"Manera\", \"Menos\", \"Ello\", \"La\", \"Se\", \"Así\", \"Con\", \"Para\", \"En\", \"Por\", \"Puedan\", \"Todas\", \"Fin\", \"Si\", \"Asimismo\", \"000\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "unidas_podemos_phrases_count = cl.delete_extra_words(unidas_podemos_phrases_count, to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "unidas_podemos_phrases_count = cl.for_streamlit(unidas_podemos_phrases_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unidas_podemos_phrases_count.to_csv(\"./data/programs/csv-s/unidas_podemos_phrases_count.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "unidas_podemos_verbs = cl.spacy_verb(unidas_podemos_phrases, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unidas_podemos_verbs.to_csv(\"./data/programs/csv-s/unidas_podemos_verbs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "unidas_podemos_nouns = cl.spacy_noun(unidas_podemos_phrases, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unidas_podemos_nouns.to_csv(\"./data/programs/csv-s/unidas_podemos_nouns.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ciudadanos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciudadanos_phrases = gd.phrases(\"Ciudadanos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciudadanos_phrases['tokenized'] = ciudadanos_phrases['phrases'].apply(cl.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciudadanos_phrases['tokenized'] = ciudadanos_phrases['tokenized'].apply(cl.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciudadanos_phrases_count = cl.count_values(ciudadanos_phrases, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"Los\", \"Impulsaremos\", \"Garantizaremos\", \"Apostaremos\",\n",
    "            \"Través\", \"Puedan\", \"Aseguraremos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciudadanos_phrases_count = cl.delete_extra_words(ciudadanos_phrases_count, to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciudadanos_phrases_count = cl.for_streamlit(ciudadanos_phrases_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ciudadanos_phrases_count.to_csv(\"./data/programs/csv-s/ciudadanos_phrases_count.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciudadanos_verbs = cl.spacy_verb(ciudadanos_phrases, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ciudadanos_verbs.to_csv(\"./data/programs/csv-s/ciudadanos_verbs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciudadanos_nouns = cl.spacy_noun(ciudadanos_phrases, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ciudadanos_nouns.to_csv(\"./data/programs/csv-s/ciudadanos_nouns.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "vox_phrases = gd.phrases(\"Vox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "vox_phrases['tokenized'] = vox_phrases['phrases'].apply(cl.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "vox_phrases['tokenized'] = vox_phrases['tokenized'].apply(cl.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "vox_phrases_count = cl.count_values(vox_phrases, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"Deben\", \"Queda\",\n",
    "             \"Frente\", \"Especialmente\", \"Cometan\", \"Su\", \"Pequeñas\", \"Adecuado\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "vox_phrases_count = cl.delete_extra_words(vox_phrases_count, to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "vox_phrases_count = cl.for_streamlit(vox_phrases_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vox_phrases_count.to_csv(\"./data/programs/csv-s/vox_phrases_count.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "vox_verbs = cl.spacy_verb(vox_phrases, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vox_verbs.to_csv(\"./data/programs/csv-s/vox_verbs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "vox_nouns = cl.spacy_noun(vox_phrases, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vox_nouns.to_csv(\"./data/programs/csv-s/vox_nouns.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VERBA RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ángel Gabilondo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_gabilondo = gd.verba_candidate(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_gabilondo['tokenized'] = verba_gabilondo['phrase'].apply(cl.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_gabilondo['tokenized'] = verba_gabilondo['tokenized'].apply(cl.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_gabilondo_count = cl.count_values(verba_gabilondo, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = [\"El\", \"Si\", \"Y\", \"La\", \"Dice\", \"Hoy\", \"Presentado\", \"En\", \"Va\", \"Hora\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_gabilondo_count = cl.delete_extra_words(verba_gabilondo_count, to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_gabilondo_count = cl.for_streamlit(verba_gabilondo_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verba_gabilondo_count.to_csv(\"./data/verba_results/verba_candidates/gabilondo_verba.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edmundo Bal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_bal = gd.verba_candidate(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_bal['tokenized'] = verba_bal['phrase'].apply(cl.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_bal['tokenized'] = verba_bal['tokenized'].apply(cl.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_bal = cl.count_values(verba_bal, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = [\"El\", \"Y\", \"La\", \"Dice\", \"Tres\", \"Hoy\", \"En\", \"Si\", \"Tras\", \"Se\", \"Mañana\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_bal_count = cl.delete_extra_words(verba_bal, to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_bal_count = cl.for_streamlit(verba_bal_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verba_bal_count.to_csv(\"./data/verba_results/verba_candidates/bal_verba.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isabel Díaz Ayuso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_ayuso = gd.verba_candidate(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_ayuso['tokenized'] = verba_ayuso['phrase'].apply(cl.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_ayuso['tokenized'] = verba_ayuso['tokenized'].apply(cl.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_ayuso = cl.count_values(verba_ayuso, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = [\"El\", \"Y\", \"La\", \"Dice\", \"Dos\", \"Hoy\", \"Si\", \"En\", \"No\", \"Sido\", \"Los\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_ayuso_count = cl.delete_extra_words(verba_ayuso, to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_ayuso_count = cl.for_streamlit(verba_ayuso_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verba_ayuso_count.to_csv(\"./data/verba_results/verba_candidates/ayuso_verba.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mónica García "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_garcia = gd.verba_candidate(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_garcia['tokenized'] = verba_garcia['phrase'].apply(cl.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_garcia['tokenized'] = verba_garcia['tokenized'].apply(cl.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_garcia = cl.count_values(verba_garcia, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = [\"La\", \"En\", \"Días\", \"El\", \"Y\", \"4\", \"Hoy\", \"Es\", \"Años\", \"Semana\", \"Ciento\", \"Real\", \"Por\", \"Si\", \"Solo\", \"A\", \"Dos\", \"Va\", \"Dice\", \"Ahora\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_garcia_count = cl.delete_extra_words(verba_garcia, to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_garcia_count = cl.for_streamlit(verba_garcia_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verba_garcia_count.to_csv(\"./data/verba_results/verba_candidates/garcia_verba.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pablo Iglesias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_iglesias = gd.verba_candidate(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_iglesias['tokenized'] = verba_iglesias['phrase'].apply(cl.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_iglesias['tokenized'] = verba_iglesias['phrase'].apply(cl.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_iglesias['tokenized'] = verba_iglesias['tokenized'].apply(cl.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_iglesias = cl.count_values(verba_iglesias, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = [\"El\", \"Y\", \"Más\", \"Hoy\", \"Después\", \"La\", \"En\", \"Va\", \"Dice\", \"Aunque\", \"Si\", \"Dos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_iglesias_count = cl.delete_extra_words(verba_iglesias, to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "verba_iglesias_count = cl.for_streamlit(verba_iglesias_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verba_iglesias_count.to_csv(\"./data/verba_results/verba_candidates/iglesias_verba.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rocío Monasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_monasterio = gd.verba_candidate(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_monasterio['tokenized'] = verba_monasterio['phrase'].apply(cl.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_monasterio['tokenized'] = verba_monasterio['tokenized'].apply(cl.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_monasterio = cl.count_values(verba_monasterio, 'tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = [\"El\", \"La\", \"Y\", \"Tres\", \"Más\", \"Dice\", \"Si\", \"Ser\", \"Quiere\", \"En\", \"Hoy\", \"Ciento\", \"Va\", \"A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_monasterio_count = cl.delete_extra_words(verba_monasterio, to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "verba_monasterio_count = cl.for_streamlit(verba_monasterio_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verba_monasterio_count.to_csv(\"./data/verba_results/verba_candidates/monasterio_verba.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy",
   "language": "python",
   "name": "spacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
